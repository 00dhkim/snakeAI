{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake_gym import SnakeGym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.0025\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.1\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 2000\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 2000\n",
    "        self.memory = deque(maxlen=50000)\n",
    "\n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_state_dict(torch.load(\n",
    "                './snake_dqn_trained.bin'))\n",
    "    \n",
    "    # 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "    def build_model(self):\n",
    "        # model = nn.Sequential(\n",
    "        #     nn.Linear(self.state_size, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(64, 32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(32, 32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(32, self.action_size),\n",
    "        # )\n",
    "        cnn_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, 5),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 32, 3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 64, 3),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(256, 64),\n",
    "                nn.Linear(64, self.action_size),\n",
    "                )\n",
    "        return cnn_model\n",
    "\n",
    "    # 타깃 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # 무작위 행동 반환\n",
    "            return torch.LongTensor([[random.randrange(4)]])\n",
    "        else:\n",
    "            # 모델로부터 행동 산출\n",
    "            return self.model(state).data.max(1)[1].view(1, 1)\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        # reward = torch.FloatTensor([reward])\n",
    "        # next_state = torch.FloatTensor([next_state])\n",
    "        # done = torch.FloatTensor([done])\n",
    "        reward = torch.FloatTensor(np.array([reward]))\n",
    "        next_state = torch.FloatTensor(np.array([next_state]))\n",
    "        done = torch.FloatTensor(np.array([done]))\n",
    "\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "        dones = torch.cat(dones)\n",
    "\n",
    "        # 현재 상태에 대한 모델의 큐함수\n",
    "        # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        max_next_q = self.target_model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (self.discount_factor * max_next_q)\n",
    "\n",
    "        # 벨만 최적 방정식을 이용한 업데이트 타깃\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = random.sample(agent.memory, agent.batch_size) # 32\n",
    "# states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "# # print(states)\n",
    "# print(len(states), states[0].shape) # torch.Size([1, 3, 10, 10])\n",
    "# states = torch.cat(states)\n",
    "# actions = torch.cat(actions)\n",
    "# rewards = torch.cat(rewards)\n",
    "# next_states = torch.cat(next_states)\n",
    "# dones = torch.cat(dones)\n",
    "\n",
    "# print(states)\n",
    "# print(states.shape) # torch.Size([32, 3, 10, 10])\n",
    "# print(agent.model)\n",
    "\n",
    "# agent.model(states) #ERROR!\n",
    "\n",
    "# # current_q = agent.model(states).gather(1, actions)\n",
    "# # max_next_q = agent.target_model(next_states).detach().max(1)[0]\n",
    "# # expected_q = rewards + (agent.discount_factor * max_next_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 4\n"
     ]
    }
   ],
   "source": [
    "env = SnakeGym()\n",
    "state_size = env.state_size\n",
    "action_size = env.action_size\n",
    "\n",
    "print(state_size, action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 500000\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "scores, episodes = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: -100   memory length: 1   epsilon: 1.0\n",
      "episode: 1   score: -99   memory length: 17   epsilon: 1.0\n",
      "episode: 2   score: -100   memory length: 18   epsilon: 1.0\n",
      "episode: 3   score: -99   memory length: 45   epsilon: 1.0\n",
      "episode: 4   score: -100   memory length: 48   epsilon: 1.0\n",
      "episode: 5   score: -100   memory length: 52   epsilon: 1.0\n",
      "episode: 6   score: -100   memory length: 54   epsilon: 1.0\n",
      "episode: 7   score: -100   memory length: 71   epsilon: 1.0\n",
      "episode: 8   score: -99   memory length: 83   epsilon: 1.0\n",
      "episode: 9   score: -100   memory length: 102   epsilon: 1.0\n",
      "episode: 10   score: -100   memory length: 108   epsilon: 1.0\n",
      "episode: 11   score: -100   memory length: 112   epsilon: 1.0\n",
      "episode: 12   score: -100   memory length: 113   epsilon: 1.0\n",
      "episode: 13   score: -100   memory length: 114   epsilon: 1.0\n",
      "episode: 14   score: -100   memory length: 119   epsilon: 1.0\n",
      "episode: 15   score: -100   memory length: 152   epsilon: 1.0\n",
      "episode: 16   score: -100   memory length: 175   epsilon: 1.0\n",
      "episode: 17   score: -100   memory length: 178   epsilon: 1.0\n",
      "episode: 18   score: -100   memory length: 201   epsilon: 1.0\n",
      "episode: 19   score: -100   memory length: 248   epsilon: 1.0\n",
      "episode: 20   score: -100   memory length: 249   epsilon: 1.0\n",
      "episode: 21   score: -100   memory length: 251   epsilon: 1.0\n",
      "episode: 22   score: -100   memory length: 259   epsilon: 1.0\n",
      "episode: 23   score: -100   memory length: 270   epsilon: 1.0\n",
      "episode: 24   score: -100   memory length: 284   epsilon: 1.0\n",
      "episode: 25   score: -100   memory length: 295   epsilon: 1.0\n",
      "episode: 26   score: -100   memory length: 315   epsilon: 1.0\n",
      "episode: 27   score: -99   memory length: 326   epsilon: 1.0\n",
      "episode: 28   score: -99   memory length: 343   epsilon: 1.0\n",
      "episode: 29   score: -100   memory length: 345   epsilon: 1.0\n",
      "episode: 30   score: -99   memory length: 366   epsilon: 1.0\n",
      "episode: 31   score: -100   memory length: 377   epsilon: 1.0\n",
      "episode: 32   score: -100   memory length: 378   epsilon: 1.0\n",
      "episode: 33   score: -100   memory length: 382   epsilon: 1.0\n",
      "episode: 34   score: -100   memory length: 386   epsilon: 1.0\n",
      "episode: 35   score: -100   memory length: 397   epsilon: 1.0\n",
      "episode: 36   score: -100   memory length: 400   epsilon: 1.0\n",
      "episode: 37   score: -100   memory length: 404   epsilon: 1.0\n",
      "episode: 38   score: -100   memory length: 411   epsilon: 1.0\n",
      "episode: 39   score: -100   memory length: 435   epsilon: 1.0\n",
      "episode: 40   score: -100   memory length: 517   epsilon: 1.0\n",
      "episode: 41   score: -100   memory length: 532   epsilon: 1.0\n",
      "episode: 42   score: -100   memory length: 537   epsilon: 1.0\n",
      "episode: 43   score: -100   memory length: 538   epsilon: 1.0\n",
      "episode: 44   score: -99   memory length: 547   epsilon: 1.0\n",
      "episode: 45   score: -99   memory length: 566   epsilon: 1.0\n",
      "episode: 46   score: -100   memory length: 587   epsilon: 1.0\n",
      "episode: 47   score: -100   memory length: 602   epsilon: 1.0\n",
      "episode: 48   score: -100   memory length: 624   epsilon: 1.0\n",
      "episode: 49   score: -100   memory length: 656   epsilon: 1.0\n",
      "episode: 50   score: -100   memory length: 668   epsilon: 1.0\n",
      "episode: 51   score: -100   memory length: 679   epsilon: 1.0\n",
      "episode: 52   score: -100   memory length: 689   epsilon: 1.0\n",
      "episode: 53   score: -100   memory length: 751   epsilon: 1.0\n",
      "episode: 54   score: -100   memory length: 802   epsilon: 1.0\n",
      "episode: 55   score: -100   memory length: 807   epsilon: 1.0\n",
      "episode: 56   score: -100   memory length: 810   epsilon: 1.0\n",
      "episode: 57   score: -100   memory length: 811   epsilon: 1.0\n",
      "episode: 58   score: -100   memory length: 832   epsilon: 1.0\n",
      "episode: 59   score: -100   memory length: 879   epsilon: 1.0\n",
      "episode: 60   score: -100   memory length: 880   epsilon: 1.0\n",
      "episode: 61   score: -100   memory length: 882   epsilon: 1.0\n",
      "episode: 62   score: -100   memory length: 887   epsilon: 1.0\n",
      "episode: 63   score: -100   memory length: 914   epsilon: 1.0\n",
      "episode: 64   score: -100   memory length: 921   epsilon: 1.0\n",
      "episode: 65   score: -100   memory length: 924   epsilon: 1.0\n",
      "episode: 66   score: -100   memory length: 926   epsilon: 1.0\n",
      "episode: 67   score: -100   memory length: 931   epsilon: 1.0\n",
      "episode: 68   score: -100   memory length: 940   epsilon: 1.0\n",
      "episode: 69   score: -100   memory length: 942   epsilon: 1.0\n",
      "episode: 70   score: -100   memory length: 960   epsilon: 1.0\n",
      "episode: 71   score: -100   memory length: 963   epsilon: 1.0\n",
      "episode: 72   score: -99   memory length: 966   epsilon: 1.0\n",
      "episode: 73   score: -100   memory length: 975   epsilon: 1.0\n",
      "episode: 74   score: -100   memory length: 1002   epsilon: 1.0\n",
      "episode: 75   score: -100   memory length: 1007   epsilon: 1.0\n",
      "episode: 76   score: -100   memory length: 1008   epsilon: 1.0\n",
      "episode: 77   score: -100   memory length: 1012   epsilon: 1.0\n",
      "episode: 78   score: -100   memory length: 1020   epsilon: 1.0\n",
      "episode: 79   score: -100   memory length: 1033   epsilon: 1.0\n",
      "episode: 80   score: -100   memory length: 1039   epsilon: 1.0\n",
      "episode: 81   score: -100   memory length: 1040   epsilon: 1.0\n",
      "episode: 82   score: -100   memory length: 1057   epsilon: 1.0\n",
      "episode: 83   score: -100   memory length: 1059   epsilon: 1.0\n",
      "episode: 84   score: -100   memory length: 1063   epsilon: 1.0\n",
      "episode: 85   score: -100   memory length: 1064   epsilon: 1.0\n",
      "episode: 86   score: -100   memory length: 1069   epsilon: 1.0\n",
      "episode: 87   score: -100   memory length: 1073   epsilon: 1.0\n",
      "episode: 88   score: -100   memory length: 1092   epsilon: 1.0\n",
      "episode: 89   score: -100   memory length: 1113   epsilon: 1.0\n",
      "episode: 90   score: -99   memory length: 1126   epsilon: 1.0\n",
      "episode: 91   score: -100   memory length: 1135   epsilon: 1.0\n",
      "episode: 92   score: -100   memory length: 1138   epsilon: 1.0\n",
      "episode: 93   score: -100   memory length: 1143   epsilon: 1.0\n",
      "episode: 94   score: -100   memory length: 1178   epsilon: 1.0\n",
      "episode: 95   score: -100   memory length: 1203   epsilon: 1.0\n",
      "episode: 96   score: -100   memory length: 1218   epsilon: 1.0\n",
      "episode: 97   score: -100   memory length: 1221   epsilon: 1.0\n",
      "episode: 98   score: -100   memory length: 1233   epsilon: 1.0\n",
      "episode: 99   score: -100   memory length: 1234   epsilon: 1.0\n",
      "episode: 100   score: -100   memory length: 1241   epsilon: 1.0\n",
      "episode: 101   score: -100   memory length: 1261   epsilon: 1.0\n",
      "episode: 102   score: -100   memory length: 1309   epsilon: 1.0\n",
      "episode: 103   score: -100   memory length: 1322   epsilon: 1.0\n",
      "episode: 104   score: -100   memory length: 1334   epsilon: 1.0\n",
      "episode: 105   score: -99   memory length: 1352   epsilon: 1.0\n",
      "episode: 106   score: -99   memory length: 1393   epsilon: 1.0\n",
      "episode: 107   score: -100   memory length: 1401   epsilon: 1.0\n",
      "episode: 108   score: -100   memory length: 1414   epsilon: 1.0\n",
      "episode: 109   score: -100   memory length: 1415   epsilon: 1.0\n",
      "episode: 110   score: -100   memory length: 1432   epsilon: 1.0\n",
      "episode: 111   score: -100   memory length: 1444   epsilon: 1.0\n",
      "episode: 112   score: -99   memory length: 1449   epsilon: 1.0\n",
      "episode: 113   score: -100   memory length: 1450   epsilon: 1.0\n",
      "episode: 114   score: -100   memory length: 1495   epsilon: 1.0\n",
      "episode: 115   score: -100   memory length: 1525   epsilon: 1.0\n",
      "episode: 116   score: -99   memory length: 1550   epsilon: 1.0\n",
      "episode: 117   score: -100   memory length: 1578   epsilon: 1.0\n",
      "episode: 118   score: -100   memory length: 1614   epsilon: 1.0\n",
      "episode: 119   score: -100   memory length: 1619   epsilon: 1.0\n",
      "episode: 120   score: -100   memory length: 1622   epsilon: 1.0\n",
      "episode: 121   score: -100   memory length: 1627   epsilon: 1.0\n",
      "episode: 122   score: -100   memory length: 1633   epsilon: 1.0\n",
      "episode: 123   score: -100   memory length: 1639   epsilon: 1.0\n",
      "episode: 124   score: -99   memory length: 1642   epsilon: 1.0\n",
      "episode: 125   score: -100   memory length: 1648   epsilon: 1.0\n",
      "episode: 126   score: -99   memory length: 1659   epsilon: 1.0\n",
      "episode: 127   score: -100   memory length: 1670   epsilon: 1.0\n",
      "episode: 128   score: -100   memory length: 1684   epsilon: 1.0\n",
      "episode: 129   score: -100   memory length: 1689   epsilon: 1.0\n",
      "episode: 130   score: -100   memory length: 1692   epsilon: 1.0\n",
      "episode: 131   score: -100   memory length: 1698   epsilon: 1.0\n",
      "episode: 132   score: -100   memory length: 1701   epsilon: 1.0\n",
      "episode: 133   score: -100   memory length: 1711   epsilon: 1.0\n",
      "episode: 134   score: -100   memory length: 1715   epsilon: 1.0\n",
      "episode: 135   score: -100   memory length: 1727   epsilon: 1.0\n",
      "episode: 136   score: -100   memory length: 1740   epsilon: 1.0\n",
      "episode: 137   score: -100   memory length: 1747   epsilon: 1.0\n",
      "episode: 138   score: -99   memory length: 1795   epsilon: 1.0\n",
      "episode: 139   score: -100   memory length: 1796   epsilon: 1.0\n",
      "episode: 140   score: -100   memory length: 1803   epsilon: 1.0\n",
      "episode: 141   score: -100   memory length: 1805   epsilon: 1.0\n",
      "episode: 142   score: -100   memory length: 1808   epsilon: 1.0\n",
      "episode: 143   score: -100   memory length: 1809   epsilon: 1.0\n",
      "episode: 144   score: -100   memory length: 1811   epsilon: 1.0\n",
      "episode: 145   score: -100   memory length: 1815   epsilon: 1.0\n",
      "episode: 146   score: -100   memory length: 1822   epsilon: 1.0\n",
      "episode: 147   score: -100   memory length: 1823   epsilon: 1.0\n",
      "episode: 148   score: -100   memory length: 1827   epsilon: 1.0\n",
      "episode: 149   score: -99   memory length: 1844   epsilon: 1.0\n",
      "episode: 150   score: -100   memory length: 1850   epsilon: 1.0\n",
      "episode: 151   score: -100   memory length: 1855   epsilon: 1.0\n",
      "episode: 152   score: -100   memory length: 1864   epsilon: 1.0\n",
      "episode: 153   score: -100   memory length: 1867   epsilon: 1.0\n",
      "episode: 154   score: -100   memory length: 1871   epsilon: 1.0\n",
      "episode: 155   score: -100   memory length: 1875   epsilon: 1.0\n",
      "episode: 156   score: -100   memory length: 1881   epsilon: 1.0\n",
      "episode: 157   score: -100   memory length: 1882   epsilon: 1.0\n",
      "episode: 158   score: -100   memory length: 1884   epsilon: 1.0\n",
      "episode: 159   score: -100   memory length: 1886   epsilon: 1.0\n",
      "episode: 160   score: -100   memory length: 1931   epsilon: 1.0\n",
      "episode: 161   score: -100   memory length: 1937   epsilon: 1.0\n",
      "episode: 162   score: -100   memory length: 1944   epsilon: 1.0\n",
      "episode: 163   score: -100   memory length: 1947   epsilon: 1.0\n",
      "episode: 164   score: -100   memory length: 1948   epsilon: 1.0\n",
      "episode: 165   score: -100   memory length: 1959   epsilon: 1.0\n",
      "episode: 166   score: -100   memory length: 1989   epsilon: 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "input must be 0, 1, 2, 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kimdo\\snakeAI\\Python\\snake_dqn.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimdo/snakeAI/Python/snake_dqn.ipynb#ch0000006?line=10'>11</a>\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(np\u001b[39m.\u001b[39marray([state]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimdo/snakeAI/Python/snake_dqn.ipynb#ch0000006?line=11'>12</a>\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(state)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kimdo/snakeAI/Python/snake_dqn.ipynb#ch0000006?line=13'>14</a>\u001b[0m next_state, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimdo/snakeAI/Python/snake_dqn.ipynb#ch0000006?line=15'>16</a>\u001b[0m agent\u001b[39m.\u001b[39mappend_sample(state, action, reward, next_state, done)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kimdo/snakeAI/Python/snake_dqn.ipynb#ch0000006?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m agent\u001b[39m.\u001b[39mtrain_start:\n",
      "File \u001b[1;32mc:\\Users\\kimdo\\snakeAI\\Python\\snake_gym.py:140\u001b[0m, in \u001b[0;36mSnakeGym.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    138\u001b[0m     direc \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39minput must be 0, 1, 2, 3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    142\u001b[0m i, j \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msnake\u001b[39m.\u001b[39mhead_pos()[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m direc[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msnake\u001b[39m.\u001b[39mhead_pos()[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m direc[\u001b[39m1\u001b[39m]\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap[i][j] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFOOD: \u001b[39m# FOOD\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: input must be 0, 1, 2, 3"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        # if e % 100 == 0:\n",
    "            # env.render()\n",
    "        \n",
    "        # state = torch.FloatTensor([state]) # 이렇게 하면 느리다고 워닝뜸\n",
    "        state = torch.FloatTensor(np.array([state]))\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        if len(agent.memory) > agent.train_start:\n",
    "            agent.train_model()\n",
    "        \n",
    "        score += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, scores, 'b')\n",
    "            pylab.savefig('./snake_dqn.png')\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
